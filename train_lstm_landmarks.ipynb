{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GestureDataset\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m \n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolutions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msolutions\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtasks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"MediaPipe Tasks Audio API.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_classifier\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_embedder\u001b[39;00m\n\u001b[0;32m     21\u001b[0m AudioClassifier \u001b[38;5;241m=\u001b[39m audio_classifier\u001b[38;5;241m.\u001b[39mAudioClassifier\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\audio_classifier.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_options_pb2\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_audio_task_api\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_data \u001b[38;5;28;01mas\u001b[39;00m audio_data_module\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_result \u001b[38;5;28;01mas\u001b[39;00m classification_result_module\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\audio\\core\\base_audio_task_api.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_record\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio_task_running_mode \u001b[38;5;28;01mas\u001b[39;00m running_mode_module\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptional_dependencies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     27\u001b[0m _TaskRunner \u001b[38;5;241m=\u001b[39m task_runner_module\u001b[38;5;241m.\u001b[39mTaskRunner\n\u001b[0;32m     28\u001b[0m _Packet \u001b[38;5;241m=\u001b[39m packet_module\u001b[38;5;241m.\u001b[39mPacket\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\core\\optional_dependencies.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# TensorFlow isn't a dependency of mediapipe pip package. It's only\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# required in the API docgen pipeline so we'll ignore it if tensorflow is not\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# installed.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doc_controls\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;66;03m# Replace the real doc_controls.do_not_generate_docs with an no-op\u001b[39;00m\n\u001b[0;32m     23\u001b[0m   doc_controls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     46\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:206\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:130\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpad_to_cardinality\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_to_cardinality\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsing_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_example_dataset\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprefetching_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_to_device\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprefetching_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefetch_to_device\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom_access\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m at\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1430\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1402\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1535\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.utils import process_batch_for_landmarks, extract_landmarks_from_batch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from dataset.loader import GestureDataset\n",
    "import mediapipe as mp \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "file = os.path.join(\"D:\", \"IPN_Hand\",\"annotations-20231128T085307Z-001\", \"annotations\", \"Annot_TrainList.txt\")\n",
    "# frame_folders = os.path.join(\".\", \"IPN_Hand\", \"frames\")\n",
    "hdf5_path = os.path.join(\"D:\", \"IPN_Hand\", \"hand_gestures.h5\")\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "dataset = GestureDataset(hdf5_path, file, label_all_gestures=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 60, 3, 240, 320])\n",
      "(5, 60, 63)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data[0].shape)\n",
    "    processed_batch = process_batch_for_landmarks(data[0])\n",
    "    landmarks = extract_landmarks_from_batch(processed_batch, hands)\n",
    "    print(landmarks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LSTM import LSTMGestureClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMGestureClassifier(63, 128, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 60, 63])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    processed_batch = process_batch_for_landmarks(data[0])\n",
    "    landmarks = extract_landmarks_from_batch(processed_batch, hands)\n",
    "    landmarks = torch.tensor(landmarks).float()\n",
    "    print(landmarks.shape)\n",
    "    out = model(landmarks)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_no_gesture = 0.8  # Example value\n",
    "weight_for_gesture = 0.2     # Example value\n",
    "weights = torch.tensor([weight_for_gesture, weight_for_no_gesture])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 0.6907\n",
      "Epoch [1], Loss: 0.4617\n",
      "Epoch [1], Loss: 3.9959\n",
      "Epoch [1], Loss: 1.3541\n",
      "Epoch [1], Loss: 1.8308\n",
      "Epoch [1], Loss: 0.4905\n",
      "Epoch [1], Loss: 0.6959\n",
      "Epoch [1], Loss: 0.7028\n",
      "Epoch [1], Loss: 0.6815\n",
      "Epoch [1], Loss: 0.8254\n",
      "Epoch [1], Loss: 0.8440\n",
      "Epoch [1], Loss: 0.6485\n",
      "Epoch [1], Loss: 0.8749\n",
      "Epoch [1], Loss: 0.6034\n",
      "Epoch [1], Loss: 0.5337\n",
      "Epoch [1], Loss: 0.5783\n",
      "Epoch [1], Loss: 0.7619\n",
      "Epoch [1], Loss: 0.4602\n",
      "Epoch [1], Loss: 0.2805\n",
      "Epoch [1], Loss: 1.1322\n",
      "Epoch [1], Loss: 0.4554\n",
      "Epoch [1], Loss: 0.6650\n",
      "Epoch [1], Loss: 0.2211\n",
      "Epoch [1], Loss: 0.9780\n",
      "Epoch [1], Loss: 0.1491\n",
      "Epoch [1], Loss: 1.0419\n",
      "Epoch [1], Loss: 0.3127\n",
      "Epoch [1], Loss: 0.1301\n",
      "Epoch [1], Loss: 0.4283\n",
      "Epoch [1], Loss: 0.8201\n",
      "Epoch [1], Loss: 0.3461\n",
      "Epoch [1], Loss: 0.5697\n",
      "Epoch [1], Loss: 0.4075\n",
      "Epoch [1], Loss: 1.6071\n",
      "Epoch [1], Loss: 0.6706\n",
      "Epoch [1], Loss: 0.5647\n",
      "Epoch [1], Loss: 0.5605\n",
      "Epoch [1], Loss: 0.1273\n",
      "Epoch [1], Loss: 0.2863\n",
      "Epoch [1], Loss: 1.1703\n",
      "Epoch [1], Loss: 1.1576\n",
      "Epoch [1], Loss: 0.5680\n",
      "Epoch [1], Loss: 0.3788\n",
      "Epoch [1], Loss: 0.8806\n",
      "Epoch [1], Loss: 0.4926\n",
      "Epoch [1], Loss: 0.6068\n",
      "Epoch [1], Loss: 0.6572\n",
      "Epoch [1], Loss: 0.5840\n",
      "Epoch [1], Loss: 0.6633\n",
      "Epoch [1], Loss: 0.7562\n",
      "Epoch [1], Loss: 0.7611\n",
      "Epoch [1], Loss: 0.6641\n",
      "Epoch [1], Loss: 0.6503\n",
      "Epoch [1], Loss: 0.5781\n",
      "Epoch [1], Loss: 0.5608\n",
      "Epoch [1], Loss: 0.6360\n",
      "Epoch [1], Loss: 0.9248\n",
      "Epoch [1], Loss: 0.6584\n",
      "Epoch [1], Loss: 0.4606\n",
      "Epoch [1], Loss: 0.5620\n",
      "Epoch [1], Loss: 0.7421\n",
      "Epoch [1], Loss: 0.4603\n",
      "Epoch [1], Loss: 0.5669\n",
      "Epoch [1], Loss: 0.8737\n",
      "Epoch [1], Loss: 0.4382\n",
      "Epoch [1], Loss: 0.6491\n",
      "Epoch [1], Loss: 0.5827\n",
      "Epoch [1], Loss: 0.4199\n",
      "Epoch [1], Loss: 0.5710\n",
      "Epoch [1], Loss: 0.8346\n",
      "Epoch [1], Loss: 0.7973\n",
      "Epoch [1], Loss: 0.4511\n",
      "Epoch [1], Loss: 0.3604\n",
      "Epoch [1], Loss: 0.7420\n",
      "Epoch [1], Loss: 0.3067\n",
      "Epoch [1], Loss: 0.6657\n",
      "Epoch [1], Loss: 0.2354\n",
      "Epoch [1], Loss: 0.2797\n",
      "Epoch [1], Loss: 0.2265\n",
      "Epoch [1], Loss: 0.2092\n",
      "Epoch [1], Loss: 0.5073\n",
      "Epoch [1], Loss: 1.0226\n",
      "Epoch [1], Loss: 0.1202\n",
      "Epoch [1], Loss: 0.2098\n",
      "Epoch [1], Loss: 0.0828\n",
      "Epoch [1], Loss: 0.2229\n",
      "Epoch [1], Loss: 0.3546\n",
      "Epoch [1], Loss: 0.6446\n",
      "Epoch [1], Loss: 0.3685\n",
      "Epoch [1], Loss: 0.2607\n",
      "Epoch [1], Loss: 0.1779\n",
      "Epoch [1], Loss: 2.0358\n",
      "Epoch [1], Loss: 0.3839\n",
      "Epoch [1], Loss: 0.6491\n",
      "Epoch [1], Loss: 1.7579\n",
      "Epoch [1], Loss: 0.2130\n",
      "Epoch [1], Loss: 0.2684\n",
      "Epoch [1], Loss: 0.9009\n",
      "Epoch [1], Loss: 0.4921\n",
      "Epoch [1], Loss: 0.7831\n",
      "Epoch [1], Loss: 0.7186\n",
      "Epoch [1], Loss: 0.8597\n",
      "Epoch [1], Loss: 0.5666\n",
      "Epoch [1], Loss: 0.4341\n",
      "Epoch [1], Loss: 0.5739\n",
      "Epoch [1], Loss: 0.6510\n",
      "Epoch [1], Loss: 0.6637\n",
      "Epoch [1], Loss: 0.8259\n",
      "Epoch [1], Loss: 0.5573\n",
      "Epoch [1], Loss: 0.3750\n",
      "Epoch [1], Loss: 0.5827\n",
      "Epoch [1], Loss: 0.2979\n",
      "Epoch [1], Loss: 0.4720\n",
      "Epoch [1], Loss: 0.4947\n",
      "Epoch [1], Loss: 1.3449\n",
      "Epoch [1], Loss: 1.1351\n",
      "Epoch [1], Loss: 0.2576\n",
      "Epoch [1], Loss: 0.2764\n",
      "Epoch [1], Loss: 0.1900\n",
      "Epoch [1], Loss: 0.4109\n",
      "Epoch [1], Loss: 0.1570\n",
      "Epoch [1], Loss: 0.8869\n",
      "Epoch [1], Loss: 0.2286\n",
      "Epoch [1], Loss: 1.1206\n",
      "Epoch [1], Loss: 0.7866\n",
      "Epoch [1], Loss: 0.1624\n",
      "Epoch [1], Loss: 0.1698\n",
      "Epoch [1], Loss: 0.3365\n",
      "Epoch [1], Loss: 0.2164\n",
      "Epoch [1], Loss: 0.2999\n",
      "Epoch [1], Loss: 0.2415\n",
      "Epoch [1], Loss: 0.2139\n",
      "Epoch [1], Loss: 0.5607\n",
      "Epoch [1], Loss: 0.1943\n",
      "Epoch [1], Loss: 0.3875\n",
      "Epoch [1], Loss: 0.8418\n",
      "Epoch [1], Loss: 0.6394\n",
      "Epoch [1], Loss: 0.3029\n",
      "Epoch [1], Loss: 0.4818\n",
      "Epoch [1], Loss: 0.2204\n",
      "Epoch [1], Loss: 0.1866\n",
      "Epoch [1], Loss: 0.2047\n",
      "Epoch [1], Loss: 1.0766\n",
      "Epoch [1], Loss: 0.1082\n",
      "Epoch [1], Loss: 0.8614\n",
      "Epoch [1], Loss: 0.9450\n",
      "Epoch [1], Loss: 1.1033\n",
      "Epoch [1], Loss: 0.2241\n",
      "Epoch [1], Loss: 0.2012\n",
      "Epoch [1], Loss: 0.2310\n",
      "Epoch [1], Loss: 0.2385\n",
      "Epoch [1], Loss: 0.2953\n",
      "Epoch [1], Loss: 0.0857\n",
      "Epoch [1], Loss: 0.9991\n",
      "Epoch [1], Loss: 0.2205\n",
      "Epoch [1], Loss: 1.0287\n",
      "Epoch [1], Loss: 0.0590\n",
      "Epoch [1], Loss: 1.3073\n",
      "Epoch [1], Loss: 0.5024\n",
      "Epoch [1], Loss: 0.4053\n",
      "Epoch [1], Loss: 0.8680\n",
      "Epoch [1], Loss: 0.3671\n",
      "Epoch [1], Loss: 0.2198\n",
      "Epoch [1], Loss: 0.8586\n",
      "Epoch [1], Loss: 0.3030\n",
      "Epoch [1], Loss: 0.1976\n",
      "Epoch [1], Loss: 0.4712\n",
      "Epoch [1], Loss: 0.7491\n",
      "Epoch [1], Loss: 0.3380\n",
      "Epoch [1], Loss: 0.2336\n",
      "Epoch [1], Loss: 0.1796\n",
      "Epoch [1], Loss: 1.1608\n",
      "Epoch [1], Loss: 0.2678\n",
      "Epoch [1], Loss: 0.4430\n",
      "Epoch [1], Loss: 0.5923\n",
      "Epoch [1], Loss: 0.8179\n",
      "Epoch [1], Loss: 0.6508\n",
      "Epoch [1], Loss: 0.6151\n",
      "Epoch [1], Loss: 0.4854\n",
      "Epoch [1], Loss: 0.2454\n",
      "Epoch [1], Loss: 0.0503\n",
      "Epoch [1], Loss: 0.3582\n",
      "Epoch [1], Loss: 0.1634\n",
      "Epoch [1], Loss: 0.1223\n",
      "Epoch [1], Loss: 0.1060\n",
      "Epoch [1], Loss: 0.1886\n",
      "Epoch [1], Loss: 0.2254\n",
      "Epoch [1], Loss: 0.0882\n",
      "Epoch [1], Loss: 0.6351\n",
      "Epoch [1], Loss: 0.0605\n",
      "Epoch [1], Loss: 1.7645\n",
      "Epoch [1], Loss: 1.4019\n",
      "Epoch [1], Loss: 0.0603\n",
      "Epoch [1], Loss: 1.5674\n",
      "Epoch [1], Loss: 0.4477\n",
      "Epoch [1], Loss: 0.0723\n",
      "Epoch [1], Loss: 0.1494\n",
      "Epoch [1], Loss: 0.4775\n",
      "Epoch [1], Loss: 0.7901\n",
      "Epoch [1], Loss: 0.5936\n",
      "Epoch [1], Loss: 0.9059\n",
      "Epoch [1], Loss: 0.2125\n",
      "Epoch [1], Loss: 0.3598\n",
      "Epoch [1], Loss: 0.1485\n",
      "Epoch [1], Loss: 0.2646\n",
      "Epoch [1], Loss: 0.2231\n",
      "Epoch [1], Loss: 0.6830\n",
      "Epoch [1], Loss: 0.2747\n",
      "Epoch [1], Loss: 0.1610\n",
      "Epoch [1], Loss: 0.5419\n",
      "Epoch [1], Loss: 0.9176\n",
      "Epoch [1], Loss: 0.2630\n",
      "Epoch [1], Loss: 0.2223\n",
      "Epoch [1], Loss: 0.1731\n",
      "Epoch [1], Loss: 0.9505\n",
      "Epoch [1], Loss: 0.6600\n",
      "Epoch [1], Loss: 0.4951\n",
      "Epoch [1], Loss: 0.2219\n",
      "Epoch [1], Loss: 0.3192\n",
      "Epoch [1], Loss: 0.6740\n",
      "Epoch [1], Loss: 0.7305\n",
      "Epoch [1], Loss: 0.3841\n",
      "Epoch [1], Loss: 0.1574\n",
      "Epoch [1], Loss: 0.4816\n",
      "Epoch [1], Loss: 0.2128\n",
      "Epoch [1], Loss: 0.0618\n",
      "Epoch [1], Loss: 0.1256\n",
      "Epoch [1], Loss: 1.0294\n",
      "Epoch [1], Loss: 0.8751\n",
      "Epoch [1], Loss: 0.9377\n",
      "Epoch [1], Loss: 0.3266\n",
      "Epoch [1], Loss: 0.7405\n",
      "Epoch [1], Loss: 0.0875\n",
      "Epoch [1], Loss: 0.0687\n",
      "Epoch [1], Loss: 0.2711\n",
      "Epoch [1], Loss: 0.1495\n",
      "Epoch [1], Loss: 0.9350\n",
      "Epoch [1], Loss: 0.5716\n",
      "Epoch [1], Loss: 0.3068\n",
      "Epoch [1], Loss: 0.6005\n",
      "Epoch [1], Loss: 0.3191\n",
      "Epoch [1], Loss: 0.2670\n",
      "Epoch [1], Loss: 0.4326\n",
      "Epoch [1], Loss: 0.1721\n",
      "Epoch [1], Loss: 0.2865\n",
      "Epoch [1], Loss: 0.2219\n",
      "Epoch [1], Loss: 1.0260\n",
      "Epoch [1], Loss: 0.3761\n",
      "Epoch [1], Loss: 0.1196\n",
      "Epoch [1], Loss: 0.1992\n",
      "Epoch [1], Loss: 0.4164\n",
      "Epoch [1], Loss: 0.1268\n",
      "Epoch [1], Loss: 0.5837\n",
      "Epoch [1], Loss: 0.1831\n",
      "Epoch [1], Loss: 0.2401\n",
      "Epoch [1], Loss: 0.4171\n",
      "Epoch [1], Loss: 0.1301\n",
      "Epoch [1], Loss: 0.2532\n",
      "Epoch [1], Loss: 0.3058\n",
      "Epoch [1], Loss: 0.1769\n",
      "Epoch [1], Loss: 1.3091\n",
      "Epoch [1], Loss: 0.0732\n",
      "Epoch [1], Loss: 1.3029\n",
      "Epoch [1], Loss: 0.0566\n",
      "Epoch [1], Loss: 0.1514\n",
      "Epoch [1], Loss: 0.1469\n",
      "Epoch [1], Loss: 0.1618\n",
      "Epoch [1], Loss: 0.1892\n",
      "Epoch [1], Loss: 0.1284\n",
      "Epoch [1], Loss: 0.6257\n",
      "Epoch [1], Loss: 0.3216\n",
      "Epoch [1], Loss: 0.1497\n",
      "Epoch [1], Loss: 0.0802\n",
      "Epoch [1], Loss: 0.1629\n",
      "Epoch [1], Loss: 0.9871\n",
      "Epoch [1], Loss: 0.8719\n",
      "Epoch [1], Loss: 0.0610\n",
      "Epoch [1], Loss: 0.1702\n",
      "Epoch [1], Loss: 0.2000\n",
      "Epoch [1], Loss: 0.4198\n",
      "Epoch [1], Loss: 0.0544\n",
      "Epoch [1], Loss: 0.1732\n",
      "Epoch [1], Loss: 0.8108\n",
      "Epoch [1], Loss: 0.9872\n",
      "Epoch [1], Loss: 0.8289\n",
      "Epoch [1], Loss: 0.0505\n",
      "Epoch [1], Loss: 0.4314\n",
      "Epoch [1], Loss: 0.1970\n",
      "Epoch [1], Loss: 1.1925\n",
      "Epoch [1], Loss: 0.0741\n",
      "Epoch [1], Loss: 0.1275\n",
      "Epoch [1], Loss: 0.7821\n",
      "Epoch [1], Loss: 0.4357\n",
      "Epoch [1], Loss: 0.2790\n",
      "Epoch [1], Loss: 0.8985\n",
      "Epoch [1], Loss: 0.1845\n",
      "Epoch [1], Loss: 0.7464\n",
      "Epoch [1], Loss: 0.3100\n",
      "Epoch [1], Loss: 0.5401\n",
      "Epoch [1], Loss: 0.1939\n",
      "Epoch [1], Loss: 0.3013\n",
      "Epoch [1], Loss: 0.6472\n",
      "Epoch [1], Loss: 0.8118\n",
      "Epoch [1], Loss: 0.2097\n",
      "Epoch [1], Loss: 0.3117\n",
      "Epoch [1], Loss: 0.3100\n",
      "Epoch [1], Loss: 0.2081\n",
      "Epoch [1], Loss: 0.2797\n",
      "Epoch [1], Loss: 0.4391\n",
      "Epoch [1], Loss: 0.2356\n",
      "Epoch [1], Loss: 0.4031\n",
      "Epoch [1], Loss: 0.6481\n",
      "Epoch [1], Loss: 0.2213\n",
      "Epoch [1], Loss: 0.1671\n",
      "Epoch [1], Loss: 0.3601\n",
      "Epoch [1], Loss: 0.4337\n",
      "Epoch [1], Loss: 1.1753\n",
      "Epoch [1], Loss: 1.0635\n",
      "Epoch [1], Loss: 0.2086\n",
      "Epoch [1], Loss: 0.1606\n",
      "Epoch [1], Loss: 0.1437\n",
      "Epoch [1], Loss: 0.9729\n",
      "Epoch [1], Loss: 0.2639\n",
      "Epoch [1], Loss: 0.1186\n",
      "Epoch [1], Loss: 0.9658\n",
      "Epoch [1], Loss: 0.8443\n",
      "Epoch [1], Loss: 0.2329\n",
      "Epoch [1], Loss: 0.1534\n",
      "Epoch [1], Loss: 0.2320\n",
      "Epoch [1], Loss: 0.1096\n",
      "Epoch [1], Loss: 1.2109\n",
      "Epoch [1], Loss: 0.2544\n",
      "Epoch [1], Loss: 0.0499\n",
      "Epoch [1], Loss: 0.0684\n",
      "Epoch [1], Loss: 0.7668\n",
      "Epoch [1], Loss: 0.3152\n",
      "Epoch [1], Loss: 0.1573\n",
      "Epoch [1], Loss: 0.1579\n",
      "Epoch [1], Loss: 0.9831\n",
      "Epoch [1], Loss: 0.3139\n",
      "Epoch [1], Loss: 0.4495\n",
      "Epoch [1], Loss: 1.2684\n",
      "Epoch [1], Loss: 0.5575\n",
      "Epoch [1], Loss: 0.2282\n",
      "Epoch [1], Loss: 0.7395\n",
      "Epoch [1], Loss: 0.9911\n",
      "Epoch [1], Loss: 0.6910\n",
      "Epoch [1], Loss: 0.2538\n",
      "Epoch [1], Loss: 0.1909\n",
      "Epoch [1], Loss: 0.6957\n",
      "Epoch [1], Loss: 0.7695\n",
      "Epoch [1], Loss: 0.1922\n",
      "Epoch [1], Loss: 0.3160\n",
      "Epoch [1], Loss: 0.1860\n",
      "Epoch [1], Loss: 1.3203\n",
      "Epoch [1], Loss: 0.1907\n",
      "Epoch [1], Loss: 0.7395\n",
      "Epoch [1], Loss: 1.8299\n",
      "Epoch [1], Loss: 0.1452\n",
      "Epoch [1], Loss: 0.4487\n",
      "Epoch [1], Loss: 0.1779\n",
      "Epoch [1], Loss: 0.6505\n",
      "Epoch [1], Loss: 0.6790\n",
      "Epoch [1], Loss: 0.3230\n",
      "Epoch [1], Loss: 0.3841\n",
      "Epoch [1], Loss: 0.5441\n",
      "Epoch [1], Loss: 0.3426\n",
      "Epoch [1], Loss: 0.8410\n",
      "Epoch [1], Loss: 0.1848\n",
      "Epoch [1], Loss: 0.2190\n",
      "Epoch [1], Loss: 0.1492\n",
      "Epoch [1], Loss: 0.2633\n",
      "Epoch [1], Loss: 1.1570\n",
      "Epoch [1], Loss: 0.2764\n",
      "Epoch [1], Loss: 0.5935\n",
      "Epoch [1], Loss: 0.8594\n",
      "Epoch [1], Loss: 0.7185\n",
      "Epoch [1], Loss: 0.9840\n",
      "Epoch [1], Loss: 0.2314\n",
      "Epoch [1], Loss: 0.5189\n",
      "Epoch [1], Loss: 0.7022\n",
      "Epoch [1], Loss: 0.2312\n",
      "Epoch [1], Loss: 0.3438\n",
      "Epoch [1], Loss: 0.4042\n",
      "Epoch [1], Loss: 1.0264\n",
      "Epoch [1], Loss: 0.3006\n",
      "Epoch [1], Loss: 0.1791\n",
      "Epoch [1], Loss: 0.2973\n",
      "Epoch [1], Loss: 0.1810\n",
      "Epoch [1], Loss: 0.3582\n",
      "Epoch [1], Loss: 0.9400\n",
      "Epoch [1], Loss: 0.1727\n",
      "Epoch [1], Loss: 0.1881\n",
      "Epoch [1], Loss: 0.1815\n",
      "Epoch [1], Loss: 0.5998\n",
      "Epoch [1], Loss: 0.8991\n",
      "Epoch [1], Loss: 0.2132\n",
      "Epoch [1], Loss: 0.7785\n",
      "Epoch [1], Loss: 0.1575\n",
      "Epoch [1], Loss: 0.5917\n",
      "Epoch [1], Loss: 0.1429\n",
      "Epoch [1], Loss: 0.1167\n",
      "Epoch [1], Loss: 0.1468\n",
      "Epoch [1], Loss: 0.3025\n",
      "Epoch [1], Loss: 0.6001\n",
      "Epoch [1], Loss: 0.8277\n",
      "Epoch [1], Loss: 0.2298\n",
      "Epoch [1], Loss: 0.4403\n",
      "Epoch [1], Loss: 0.1695\n",
      "Epoch [1], Loss: 0.1912\n",
      "Epoch [1], Loss: 0.2511\n",
      "Epoch [1], Loss: 0.1572\n",
      "Epoch [1], Loss: 0.2231\n",
      "Epoch [1], Loss: 0.1896\n",
      "Epoch [1], Loss: 1.1101\n",
      "Epoch [1], Loss: 0.8846\n",
      "Epoch [1], Loss: 0.4440\n",
      "Epoch [1], Loss: 1.1347\n",
      "Epoch [1], Loss: 0.6674\n",
      "Epoch [1], Loss: 0.8977\n",
      "Epoch [1], Loss: 0.8653\n",
      "Epoch [1], Loss: 0.1361\n",
      "Epoch [1], Loss: 0.6496\n",
      "Epoch [1], Loss: 0.0980\n",
      "Epoch [1], Loss: 0.3586\n",
      "Epoch [1], Loss: 0.3111\n",
      "Epoch [1], Loss: 0.3754\n",
      "Epoch [1], Loss: 0.2051\n",
      "Epoch [1], Loss: 0.1283\n",
      "Epoch [1], Loss: 0.7407\n",
      "Epoch [1], Loss: 0.3975\n",
      "Epoch [1], Loss: 0.1046\n",
      "Epoch [1], Loss: 0.3913\n",
      "Epoch [1], Loss: 0.3167\n",
      "Epoch [1], Loss: 0.1777\n",
      "Epoch [1], Loss: 0.3911\n",
      "Epoch [1], Loss: 0.4204\n",
      "Epoch [1], Loss: 1.2964\n",
      "Epoch [1], Loss: 0.2319\n",
      "Epoch [1], Loss: 0.0844\n",
      "Epoch [1], Loss: 0.4825\n",
      "Epoch [1], Loss: 0.2160\n",
      "Epoch [1], Loss: 0.1577\n",
      "Epoch [1], Loss: 0.5004\n",
      "Epoch [1], Loss: 1.1068\n",
      "Epoch [1], Loss: 0.1514\n",
      "Epoch [1], Loss: 0.4436\n",
      "Epoch [1], Loss: 0.2338\n",
      "Epoch [1], Loss: 0.6126\n",
      "Epoch [1], Loss: 0.8417\n",
      "Epoch [1], Loss: 0.1059\n",
      "Epoch [1], Loss: 0.4100\n",
      "Epoch [1], Loss: 0.1599\n",
      "Epoch [1], Loss: 0.7552\n",
      "Epoch [1], Loss: 0.0946\n",
      "Epoch [1], Loss: 0.1876\n",
      "Epoch [1], Loss: 0.7567\n",
      "Epoch [1], Loss: 0.7523\n",
      "Epoch [1], Loss: 0.4252\n",
      "Epoch [1], Loss: 0.1459\n",
      "Epoch [1], Loss: 0.1256\n",
      "Epoch [1], Loss: 0.3645\n",
      "Epoch [1], Loss: 0.1142\n",
      "Epoch [1], Loss: 0.1291\n",
      "Epoch [1], Loss: 0.1221\n",
      "Epoch [1], Loss: 0.7382\n",
      "Epoch [1], Loss: 0.6539\n",
      "Epoch [1], Loss: 0.1671\n",
      "Epoch [1], Loss: 0.8748\n",
      "Epoch [1], Loss: 1.9565\n",
      "Epoch [1], Loss: 1.3259\n",
      "Epoch [1], Loss: 0.6308\n",
      "Epoch [1], Loss: 0.7190\n",
      "Epoch [1], Loss: 0.2439\n",
      "Epoch [1], Loss: 0.1263\n",
      "Epoch [1], Loss: 0.1441\n",
      "Epoch [1], Loss: 1.2171\n",
      "Epoch [1], Loss: 0.2833\n",
      "Epoch [1], Loss: 0.8569\n",
      "Epoch [1], Loss: 1.2818\n",
      "Epoch [1], Loss: 0.7887\n",
      "Epoch [1], Loss: 0.8606\n",
      "Epoch [1], Loss: 0.6804\n",
      "Epoch [1], Loss: 0.2884\n",
      "Epoch [1], Loss: 0.4300\n",
      "Epoch [1], Loss: 0.6415\n",
      "Epoch [1], Loss: 0.1535\n",
      "Epoch [1], Loss: 0.4425\n",
      "Epoch [1], Loss: 0.3412\n",
      "Epoch [1], Loss: 0.3737\n",
      "Epoch [1], Loss: 0.1840\n",
      "Epoch [1], Loss: 0.7135\n",
      "Epoch [1], Loss: 0.5073\n",
      "Epoch [1], Loss: 0.2604\n",
      "Epoch [1], Loss: 0.6581\n",
      "Epoch [1], Loss: 0.5368\n",
      "Epoch [1], Loss: 1.1568\n",
      "Epoch [1], Loss: 0.7795\n",
      "Epoch [1], Loss: 0.7075\n",
      "Epoch [1], Loss: 0.4908\n",
      "Epoch [1], Loss: 0.3772\n",
      "Epoch [1], Loss: 0.3563\n",
      "Epoch [1], Loss: 0.4651\n",
      "Epoch [1], Loss: 0.5686\n",
      "Epoch [1], Loss: 0.3295\n",
      "Epoch [1], Loss: 0.0832\n",
      "Epoch [1], Loss: 0.2158\n",
      "Epoch [1], Loss: 0.2450\n",
      "Epoch [1], Loss: 0.1341\n",
      "Epoch [1], Loss: 0.3813\n",
      "Epoch [1], Loss: 0.1202\n",
      "Epoch [1], Loss: 0.3059\n",
      "Epoch [1], Loss: 0.1605\n",
      "Epoch [1], Loss: 0.1411\n",
      "Epoch [1], Loss: 1.1733\n",
      "Epoch [1], Loss: 0.2141\n",
      "Epoch [1], Loss: 0.2793\n",
      "Epoch [1], Loss: 1.1877\n",
      "Epoch [1], Loss: 1.3551\n",
      "Epoch [1], Loss: 0.1193\n",
      "Epoch [1], Loss: 0.1180\n",
      "Epoch [1], Loss: 0.1350\n",
      "Epoch [1], Loss: 0.2041\n",
      "Epoch [1], Loss: 0.8484\n",
      "Epoch [1], Loss: 0.7526\n",
      "Epoch [1], Loss: 0.1215\n",
      "Epoch [1], Loss: 0.1702\n",
      "Epoch [1], Loss: 0.1711\n",
      "Epoch [1], Loss: 0.1944\n",
      "Epoch [1], Loss: 0.7919\n",
      "Epoch [1], Loss: 0.9687\n",
      "Epoch [1], Loss: 0.6585\n",
      "Epoch [1], Loss: 0.8699\n",
      "Epoch [1], Loss: 0.1892\n",
      "Epoch [1], Loss: 0.8020\n",
      "Epoch [1], Loss: 0.6966\n",
      "Epoch [1], Loss: 0.1820\n",
      "Epoch [1], Loss: 0.1687\n",
      "Epoch [1], Loss: 0.1086\n",
      "Epoch [1], Loss: 0.4577\n",
      "Epoch [1], Loss: 0.3997\n",
      "Epoch [1], Loss: 0.1039\n",
      "Epoch [1], Loss: 0.4044\n",
      "Epoch [1], Loss: 0.1896\n",
      "Epoch [1], Loss: 0.2368\n",
      "Epoch [1], Loss: 0.7972\n",
      "Epoch [1], Loss: 1.1383\n",
      "Epoch [1], Loss: 0.2245\n",
      "Epoch [1], Loss: 0.3210\n",
      "Epoch [1], Loss: 0.1799\n",
      "Epoch [1], Loss: 0.8037\n",
      "Epoch [1], Loss: 0.2646\n",
      "Epoch [1], Loss: 1.3564\n",
      "Epoch [1], Loss: 0.1871\n",
      "Epoch [1], Loss: 0.6155\n",
      "Epoch [1], Loss: 0.1130\n",
      "Epoch [1], Loss: 0.5548\n",
      "Epoch [1], Loss: 0.1949\n",
      "Epoch [1], Loss: 0.6969\n",
      "Epoch [1], Loss: 0.1896\n",
      "Epoch [1], Loss: 0.2514\n",
      "Epoch [1], Loss: 0.5371\n",
      "Epoch [1], Loss: 0.7883\n",
      "Epoch [1], Loss: 0.1230\n",
      "Epoch [1], Loss: 0.6179\n",
      "Epoch [1], Loss: 0.1157\n",
      "Epoch [1], Loss: 1.2666\n",
      "Epoch [1], Loss: 0.2781\n",
      "Epoch [1], Loss: 0.8388\n",
      "Epoch [1], Loss: 0.3223\n",
      "Epoch [1], Loss: 0.4890\n",
      "Epoch [1], Loss: 0.6669\n",
      "Epoch [1], Loss: 0.7031\n",
      "Epoch [1], Loss: 0.1988\n",
      "Epoch [1], Loss: 0.4101\n",
      "Epoch [1], Loss: 0.2528\n",
      "Epoch [1], Loss: 0.4342\n",
      "Epoch [1], Loss: 0.4967\n",
      "Epoch [1], Loss: 0.7758\n",
      "Epoch [1], Loss: 0.9383\n",
      "Epoch [1], Loss: 0.5068\n",
      "Epoch [1], Loss: 0.4132\n",
      "Epoch [1], Loss: 0.2256\n",
      "Epoch [1], Loss: 0.1411\n",
      "Epoch [1], Loss: 0.2408\n",
      "Epoch [1], Loss: 0.5280\n",
      "Epoch [1], Loss: 0.2909\n",
      "Epoch [1], Loss: 0.4323\n",
      "Epoch [1], Loss: 0.0769\n",
      "Epoch [1], Loss: 0.2304\n",
      "Epoch [1], Loss: 0.3338\n",
      "Epoch [1], Loss: 0.3349\n",
      "Epoch [1], Loss: 0.8170\n",
      "Epoch [1], Loss: 0.4333\n",
      "Epoch [1], Loss: 0.2623\n",
      "Epoch [1], Loss: 0.1314\n",
      "Epoch [1], Loss: 0.3977\n",
      "Epoch [1], Loss: 0.0696\n",
      "Epoch [1], Loss: 1.3215\n",
      "Epoch [1], Loss: 0.2613\n",
      "Epoch [1], Loss: 0.0743\n",
      "Epoch [1], Loss: 0.5867\n",
      "Epoch [1], Loss: 0.1667\n",
      "Epoch [1], Loss: 0.1231\n",
      "Epoch [1], Loss: 0.2712\n",
      "Epoch [1], Loss: 0.1553\n",
      "Epoch [1], Loss: 0.1053\n",
      "Epoch [1], Loss: 0.1948\n",
      "Epoch [1], Loss: 0.1577\n",
      "Epoch [1], Loss: 0.6852\n",
      "Epoch [1], Loss: 0.0875\n",
      "Epoch [1], Loss: 0.2795\n",
      "Epoch [1], Loss: 0.2012\n",
      "Epoch [1], Loss: 0.6395\n",
      "Epoch [1], Loss: 0.0781\n",
      "Epoch [1], Loss: 0.0855\n",
      "Epoch [1], Loss: 0.1049\n",
      "Epoch [1], Loss: 0.1416\n",
      "Epoch [1], Loss: 1.3872\n",
      "Epoch [1], Loss: 0.6511\n",
      "Epoch [1], Loss: 0.6466\n",
      "Epoch [1], Loss: 0.1336\n",
      "Epoch [1], Loss: 0.5242\n",
      "Epoch [1], Loss: 1.4524\n",
      "Epoch [1], Loss: 0.0773\n",
      "Epoch [1], Loss: 0.9271\n",
      "Epoch [1], Loss: 0.5491\n",
      "Epoch [1], Loss: 0.1960\n",
      "Epoch [1], Loss: 0.8308\n",
      "Epoch [1], Loss: 0.3366\n",
      "Epoch [1], Loss: 0.1535\n",
      "Epoch [1], Loss: 0.7957\n",
      "Epoch [1], Loss: 0.1058\n",
      "Epoch [1], Loss: 1.3566\n",
      "Epoch [1], Loss: 0.5073\n",
      "Epoch [1], Loss: 0.3254\n",
      "Epoch [1], Loss: 0.8502\n",
      "Epoch [1], Loss: 1.2861\n",
      "Epoch [1], Loss: 0.1767\n",
      "Epoch [1], Loss: 0.2678\n",
      "Epoch [1], Loss: 0.4738\n",
      "Epoch [1], Loss: 0.1370\n",
      "Epoch [1], Loss: 0.4504\n",
      "Epoch [1], Loss: 0.1625\n",
      "Epoch [1], Loss: 0.1721\n",
      "Epoch [1], Loss: 0.1256\n",
      "Epoch [1], Loss: 0.1806\n",
      "Epoch [1], Loss: 0.0971\n",
      "Epoch [1], Loss: 0.1452\n",
      "Epoch [1], Loss: 0.1286\n",
      "Epoch [1], Loss: 0.1827\n",
      "Epoch [1], Loss: 0.5880\n",
      "Epoch [1], Loss: 0.1053\n",
      "Epoch [1], Loss: 0.1197\n",
      "Epoch [1], Loss: 0.4960\n",
      "Epoch [1], Loss: 1.1496\n",
      "Epoch [1], Loss: 0.2758\n",
      "Epoch [1], Loss: 0.6778\n",
      "Epoch [1], Loss: 0.7287\n",
      "Epoch [1], Loss: 0.5947\n",
      "Epoch [1], Loss: 0.0916\n",
      "Epoch [1], Loss: 0.5961\n",
      "Epoch [1], Loss: 0.1404\n",
      "Epoch [1], Loss: 0.6267\n",
      "Epoch [1], Loss: 0.1934\n",
      "Epoch [1], Loss: 0.1755\n",
      "Epoch [1], Loss: 0.5667\n",
      "Epoch [1], Loss: 1.5341\n",
      "Epoch [1], Loss: 1.0669\n",
      "Epoch [1], Loss: 0.0791\n",
      "Epoch [1], Loss: 0.1190\n",
      "Epoch [1], Loss: 0.1004\n",
      "Epoch [1], Loss: 0.1190\n",
      "Epoch [1], Loss: 0.1707\n",
      "Epoch [1], Loss: 1.0264\n",
      "Epoch [1], Loss: 0.1723\n",
      "Epoch [1], Loss: 0.1542\n",
      "Epoch [1], Loss: 0.2191\n",
      "Epoch [1], Loss: 0.4522\n",
      "Epoch [1], Loss: 0.6124\n",
      "Epoch [1], Loss: 0.8157\n",
      "Epoch [1], Loss: 0.9427\n",
      "Epoch [1], Loss: 0.5946\n",
      "Epoch [1], Loss: 0.1690\n",
      "Epoch [1], Loss: 0.6965\n",
      "Epoch [1], Loss: 0.2598\n",
      "Epoch [1], Loss: 0.1799\n",
      "Epoch [1], Loss: 0.5339\n",
      "Epoch [1], Loss: 1.0863\n",
      "Epoch [1], Loss: 0.1021\n",
      "Epoch [1], Loss: 0.1531\n",
      "Epoch [1], Loss: 0.9510\n",
      "Epoch [1], Loss: 0.4302\n",
      "Epoch [1], Loss: 1.3860\n",
      "Epoch [1], Loss: 0.8923\n",
      "Epoch [1], Loss: 0.2752\n",
      "Epoch [1], Loss: 0.4387\n",
      "Epoch [1], Loss: 0.7138\n",
      "Epoch [1], Loss: 0.2621\n",
      "Epoch [1], Loss: 0.5879\n",
      "Epoch [1], Loss: 0.4496\n",
      "Epoch [1], Loss: 0.3386\n",
      "Epoch [1], Loss: 0.2146\n",
      "Epoch [1], Loss: 0.1817\n",
      "Epoch [1], Loss: 0.5411\n",
      "Epoch [1], Loss: 0.1508\n",
      "Epoch [1], Loss: 1.0715\n",
      "Epoch [1], Loss: 0.8456\n",
      "Epoch [1], Loss: 0.3254\n",
      "Epoch [1], Loss: 0.1816\n",
      "Epoch [1], Loss: 0.3189\n",
      "Epoch [1], Loss: 0.3416\n",
      "Epoch [1], Loss: 0.2045\n",
      "Epoch [1], Loss: 0.0960\n",
      "Epoch [1], Loss: 0.3127\n",
      "Epoch [1], Loss: 0.2406\n",
      "Epoch [1], Loss: 0.1432\n",
      "Epoch [1], Loss: 0.1272\n",
      "Epoch [1], Loss: 0.1352\n",
      "Epoch [1], Loss: 1.1762\n",
      "Epoch [1], Loss: 0.1270\n",
      "Epoch [1], Loss: 0.1903\n",
      "Epoch [1], Loss: 0.1934\n",
      "Epoch [1], Loss: 0.1016\n",
      "Epoch [1], Loss: 0.1395\n",
      "Epoch [1], Loss: 0.1888\n",
      "Epoch [1], Loss: 0.0728\n",
      "Epoch [1], Loss: 1.1643\n",
      "Epoch [1], Loss: 0.1969\n",
      "Epoch [1], Loss: 0.6580\n",
      "Epoch [1], Loss: 0.1026\n",
      "Epoch [1], Loss: 0.0753\n",
      "Epoch [1], Loss: 0.6520\n",
      "Epoch [1], Loss: 0.5228\n",
      "Epoch [1], Loss: 0.2692\n",
      "Epoch [1], Loss: 0.5902\n",
      "Epoch [1], Loss: 0.1078\n",
      "Epoch [1], Loss: 0.0997\n",
      "Epoch [1], Loss: 0.0729\n",
      "Epoch [1], Loss: 0.2186\n",
      "Epoch [1], Loss: 0.1226\n",
      "Epoch [1], Loss: 0.1299\n",
      "Epoch [1], Loss: 0.2136\n",
      "Epoch [1], Loss: 1.3842\n",
      "Epoch [1], Loss: 0.1136\n",
      "Epoch [1], Loss: 1.0517\n",
      "Epoch [1], Loss: 0.0788\n",
      "Epoch [1], Loss: 0.0786\n",
      "Epoch [1], Loss: 0.0564\n",
      "Epoch [1], Loss: 0.2893\n",
      "Epoch [1], Loss: 0.5751\n",
      "Epoch [1], Loss: 0.1802\n",
      "Epoch [1], Loss: 0.1213\n",
      "Epoch [1], Loss: 0.1518\n",
      "Epoch [1], Loss: 0.2938\n",
      "Epoch [1], Loss: 0.1500\n",
      "Epoch [1], Loss: 1.2095\n",
      "Epoch [1], Loss: 0.9097\n",
      "Epoch [1], Loss: 0.0754\n",
      "Epoch [1], Loss: 0.1259\n",
      "Epoch [1], Loss: 1.0095\n",
      "Epoch [1], Loss: 0.1286\n",
      "Epoch [1], Loss: 0.4668\n",
      "Epoch [1], Loss: 0.9534\n",
      "Epoch [1], Loss: 0.7899\n",
      "Epoch [1], Loss: 0.0395\n",
      "Epoch [1], Loss: 1.5656\n",
      "Epoch [1], Loss: 0.4333\n",
      "Epoch [1], Loss: 0.7623\n",
      "Epoch [1], Loss: 0.2213\n",
      "Epoch [1], Loss: 0.3708\n",
      "Epoch [1], Loss: 0.2723\n",
      "Epoch [1], Loss: 0.2374\n",
      "Epoch [1], Loss: 1.1375\n",
      "Epoch [1], Loss: 0.9780\n",
      "Epoch [1], Loss: 0.3895\n",
      "Epoch [1], Loss: 0.1585\n",
      "Epoch [1], Loss: 0.2643\n",
      "Epoch [1], Loss: 1.0010\n",
      "Epoch [1], Loss: 0.3175\n",
      "Epoch [1], Loss: 0.4126\n",
      "Epoch [1], Loss: 0.0918\n",
      "Epoch [1], Loss: 1.2804\n",
      "Epoch [1], Loss: 0.6065\n",
      "Epoch [1], Loss: 0.9988\n",
      "Epoch [1], Loss: 0.1074\n",
      "Epoch [1], Loss: 0.3509\n",
      "Epoch [1], Loss: 0.0835\n",
      "Epoch [1], Loss: 1.3408\n",
      "Epoch [1], Loss: 1.2097\n",
      "Epoch [1], Loss: 0.1303\n",
      "Epoch [1], Loss: 0.2089\n",
      "Epoch [1], Loss: 0.2036\n",
      "Epoch [1], Loss: 1.1998\n",
      "Epoch [1], Loss: 1.0719\n",
      "Epoch [1], Loss: 0.1142\n",
      "Epoch [1], Loss: 0.9617\n",
      "Epoch [1], Loss: 0.6734\n",
      "Epoch [1], Loss: 0.2011\n",
      "Epoch [1], Loss: 0.2162\n",
      "Epoch [1], Loss: 0.2173\n",
      "Epoch [1], Loss: 0.3916\n",
      "Epoch [1], Loss: 0.1798\n",
      "Epoch [1], Loss: 0.5467\n",
      "Epoch [1], Loss: 0.5744\n",
      "Epoch [1], Loss: 0.3315\n",
      "Epoch [1], Loss: 0.3410\n",
      "Epoch [1], Loss: 0.3736\n",
      "Epoch [2], Loss: 0.4617\n",
      "Epoch [2], Loss: 0.2007\n",
      "Epoch [2], Loss: 0.2987\n",
      "Epoch [2], Loss: 0.1782\n",
      "Epoch [2], Loss: 0.2909\n",
      "Epoch [2], Loss: 0.1437\n",
      "Epoch [2], Loss: 0.1292\n",
      "Epoch [2], Loss: 0.2299\n",
      "Epoch [2], Loss: 0.2128\n",
      "Epoch [2], Loss: 0.9734\n",
      "Epoch [2], Loss: 0.9822\n",
      "Epoch [2], Loss: 0.0809\n",
      "Epoch [2], Loss: 0.1337\n",
      "Epoch [2], Loss: 0.2087\n",
      "Epoch [2], Loss: 0.4792\n",
      "Epoch [2], Loss: 0.3856\n",
      "Epoch [2], Loss: 0.1742\n",
      "Epoch [2], Loss: 0.1572\n",
      "Epoch [2], Loss: 0.1621\n",
      "Epoch [2], Loss: 0.2127\n",
      "Epoch [2], Loss: 0.1076\n",
      "Epoch [2], Loss: 0.1434\n",
      "Epoch [2], Loss: 1.0523\n",
      "Epoch [2], Loss: 0.0950\n",
      "Epoch [2], Loss: 0.1000\n",
      "Epoch [2], Loss: 0.0626\n",
      "Epoch [2], Loss: 0.0571\n",
      "Epoch [2], Loss: 0.0805\n",
      "Epoch [2], Loss: 0.0609\n",
      "Epoch [2], Loss: 0.0442\n",
      "Epoch [2], Loss: 0.8738\n",
      "Epoch [2], Loss: 0.0867\n",
      "Epoch [2], Loss: 0.4437\n",
      "Epoch [2], Loss: 0.0697\n",
      "Epoch [2], Loss: 0.7981\n",
      "Epoch [2], Loss: 1.3408\n",
      "Epoch [2], Loss: 0.1109\n",
      "Epoch [2], Loss: 1.0992\n",
      "Epoch [2], Loss: 1.2070\n",
      "Epoch [2], Loss: 0.1972\n",
      "Epoch [2], Loss: 1.2450\n",
      "Epoch [2], Loss: 0.2681\n",
      "Epoch [2], Loss: 0.1461\n",
      "Epoch [2], Loss: 0.1832\n",
      "Epoch [2], Loss: 0.3322\n",
      "Epoch [2], Loss: 0.7670\n",
      "Epoch [2], Loss: 0.7058\n",
      "Epoch [2], Loss: 0.2416\n",
      "Epoch [2], Loss: 0.5004\n",
      "Epoch [2], Loss: 0.4535\n",
      "Epoch [2], Loss: 0.8197\n",
      "Epoch [2], Loss: 0.1995\n",
      "Epoch [2], Loss: 0.8187\n",
      "Epoch [2], Loss: 0.4558\n",
      "Epoch [2], Loss: 0.4227\n",
      "Epoch [2], Loss: 0.3383\n",
      "Epoch [2], Loss: 0.8010\n",
      "Epoch [2], Loss: 0.4651\n",
      "Epoch [2], Loss: 0.3089\n",
      "Epoch [2], Loss: 0.5883\n",
      "Epoch [2], Loss: 0.2755\n",
      "Epoch [2], Loss: 0.3402\n",
      "Epoch [2], Loss: 0.3237\n",
      "Epoch [2], Loss: 0.1867\n",
      "Epoch [2], Loss: 0.6723\n",
      "Epoch [2], Loss: 0.2630\n",
      "Epoch [2], Loss: 1.2140\n",
      "Epoch [2], Loss: 0.2592\n",
      "Epoch [2], Loss: 0.5532\n",
      "Epoch [2], Loss: 0.2435\n",
      "Epoch [2], Loss: 0.2350\n",
      "Epoch [2], Loss: 0.6497\n",
      "Epoch [2], Loss: 0.6794\n",
      "Epoch [2], Loss: 0.7130\n",
      "Epoch [2], Loss: 0.6091\n",
      "Epoch [2], Loss: 0.1773\n",
      "Epoch [2], Loss: 0.6120\n",
      "Epoch [2], Loss: 0.4186\n",
      "Epoch [2], Loss: 0.1285\n",
      "Epoch [2], Loss: 0.2419\n",
      "Epoch [2], Loss: 0.3511\n",
      "Epoch [2], Loss: 1.4137\n",
      "Epoch [2], Loss: 0.7368\n",
      "Epoch [2], Loss: 0.6201\n",
      "Epoch [2], Loss: 0.1158\n",
      "Epoch [2], Loss: 0.1823\n",
      "Epoch [2], Loss: 0.9207\n",
      "Epoch [2], Loss: 0.1627\n",
      "Epoch [2], Loss: 0.1718\n",
      "Epoch [2], Loss: 0.8286\n",
      "Epoch [2], Loss: 0.3906\n",
      "Epoch [2], Loss: 0.9032\n",
      "Epoch [2], Loss: 0.3793\n",
      "Epoch [2], Loss: 0.3673\n",
      "Epoch [2], Loss: 0.1403\n",
      "Epoch [2], Loss: 0.2065\n",
      "Epoch [2], Loss: 0.3535\n",
      "Epoch [2], Loss: 0.9005\n",
      "Epoch [2], Loss: 0.2468\n",
      "Epoch [2], Loss: 0.1107\n",
      "Epoch [2], Loss: 0.5065\n",
      "Epoch [2], Loss: 0.3640\n",
      "Epoch [2], Loss: 0.8983\n",
      "Epoch [2], Loss: 0.4902\n",
      "Epoch [2], Loss: 0.6547\n",
      "Epoch [2], Loss: 0.9157\n",
      "Epoch [2], Loss: 0.8783\n",
      "Epoch [2], Loss: 0.3062\n",
      "Epoch [2], Loss: 0.3624\n",
      "Epoch [2], Loss: 0.1970\n",
      "Epoch [2], Loss: 0.4217\n",
      "Epoch [2], Loss: 0.4487\n",
      "Epoch [2], Loss: 0.3634\n",
      "Epoch [2], Loss: 0.2033\n",
      "Epoch [2], Loss: 0.1285\n",
      "Epoch [2], Loss: 0.4285\n",
      "Epoch [2], Loss: 0.8787\n",
      "Epoch [2], Loss: 0.9733\n",
      "Epoch [2], Loss: 0.2746\n",
      "Epoch [2], Loss: 0.8786\n",
      "Epoch [2], Loss: 0.5421\n",
      "Epoch [2], Loss: 0.5014\n",
      "Epoch [2], Loss: 0.2975\n",
      "Epoch [2], Loss: 1.1757\n",
      "Epoch [2], Loss: 0.2636\n",
      "Epoch [2], Loss: 0.1069\n",
      "Epoch [2], Loss: 0.2092\n",
      "Epoch [2], Loss: 0.1917\n",
      "Epoch [2], Loss: 0.1697\n",
      "Epoch [2], Loss: 0.2933\n",
      "Epoch [2], Loss: 0.4710\n",
      "Epoch [2], Loss: 0.8508\n",
      "Epoch [2], Loss: 0.2926\n",
      "Epoch [2], Loss: 0.1606\n",
      "Epoch [2], Loss: 1.1703\n",
      "Epoch [2], Loss: 0.7290\n",
      "Epoch [2], Loss: 0.7062\n",
      "Epoch [2], Loss: 0.9101\n",
      "Epoch [2], Loss: 0.1889\n",
      "Epoch [2], Loss: 0.2938\n",
      "Epoch [2], Loss: 0.5530\n",
      "Epoch [2], Loss: 1.0462\n",
      "Epoch [2], Loss: 1.0727\n",
      "Epoch [2], Loss: 0.2772\n",
      "Epoch [2], Loss: 0.3896\n",
      "Epoch [2], Loss: 0.5306\n",
      "Epoch [2], Loss: 0.2856\n",
      "Epoch [2], Loss: 0.3080\n",
      "Epoch [2], Loss: 0.3435\n",
      "Epoch [2], Loss: 0.3711\n",
      "Epoch [2], Loss: 0.6902\n",
      "Epoch [2], Loss: 0.0902\n",
      "Epoch [2], Loss: 0.5527\n",
      "Epoch [2], Loss: 0.0720\n",
      "Epoch [2], Loss: 0.6082\n",
      "Epoch [2], Loss: 1.1778\n",
      "Epoch [2], Loss: 1.3080\n",
      "Epoch [2], Loss: 0.7002\n",
      "Epoch [2], Loss: 0.6899\n",
      "Epoch [2], Loss: 0.2270\n",
      "Epoch [2], Loss: 0.3759\n",
      "Epoch [2], Loss: 0.5270\n",
      "Epoch [2], Loss: 0.1884\n",
      "Epoch [2], Loss: 0.2268\n",
      "Epoch [2], Loss: 0.1925\n",
      "Epoch [2], Loss: 0.3425\n",
      "Epoch [2], Loss: 0.0892\n",
      "Epoch [2], Loss: 0.2081\n",
      "Epoch [2], Loss: 1.1741\n",
      "Epoch [2], Loss: 0.3716\n",
      "Epoch [2], Loss: 0.6097\n",
      "Epoch [2], Loss: 0.9232\n",
      "Epoch [2], Loss: 0.0624\n",
      "Epoch [2], Loss: 0.2675\n",
      "Epoch [2], Loss: 0.9403\n",
      "Epoch [2], Loss: 0.2299\n",
      "Epoch [2], Loss: 0.5971\n",
      "Epoch [2], Loss: 0.1219\n",
      "Epoch [2], Loss: 0.1406\n",
      "Epoch [2], Loss: 0.4359\n",
      "Epoch [2], Loss: 0.1331\n",
      "Epoch [2], Loss: 0.1401\n",
      "Epoch [2], Loss: 0.1519\n",
      "Epoch [2], Loss: 0.5867\n",
      "Epoch [2], Loss: 0.1835\n",
      "Epoch [2], Loss: 0.9298\n",
      "Epoch [2], Loss: 1.1480\n",
      "Epoch [2], Loss: 0.1907\n",
      "Epoch [2], Loss: 0.2201\n",
      "Epoch [2], Loss: 0.2078\n",
      "Epoch [2], Loss: 0.1603\n",
      "Epoch [2], Loss: 1.3710\n",
      "Epoch [2], Loss: 0.2467\n",
      "Epoch [2], Loss: 0.9672\n",
      "Epoch [2], Loss: 0.1066\n",
      "Epoch [2], Loss: 0.1041\n",
      "Epoch [2], Loss: 0.4995\n",
      "Epoch [2], Loss: 0.6881\n",
      "Epoch [2], Loss: 0.1954\n",
      "Epoch [2], Loss: 0.2311\n",
      "Epoch [2], Loss: 0.4704\n",
      "Epoch [2], Loss: 0.9607\n",
      "Epoch [2], Loss: 0.2200\n",
      "Epoch [2], Loss: 0.3354\n",
      "Epoch [2], Loss: 0.0843\n",
      "Epoch [2], Loss: 0.2522\n",
      "Epoch [2], Loss: 0.1818\n",
      "Epoch [2], Loss: 0.2326\n",
      "Epoch [2], Loss: 0.5622\n",
      "Epoch [2], Loss: 0.2593\n",
      "Epoch [2], Loss: 0.1025\n",
      "Epoch [2], Loss: 0.0726\n",
      "Epoch [2], Loss: 0.1532\n",
      "Epoch [2], Loss: 1.0041\n",
      "Epoch [2], Loss: 0.1291\n",
      "Epoch [2], Loss: 0.0927\n",
      "Epoch [2], Loss: 0.1592\n",
      "Epoch [2], Loss: 0.5776\n",
      "Epoch [2], Loss: 0.1221\n",
      "Epoch [2], Loss: 0.2773\n",
      "Epoch [2], Loss: 0.0519\n",
      "Epoch [2], Loss: 0.4749\n",
      "Epoch [2], Loss: 0.0961\n",
      "Epoch [2], Loss: 0.1589\n",
      "Epoch [2], Loss: 0.1083\n",
      "Epoch [2], Loss: 1.0386\n",
      "Epoch [2], Loss: 0.4786\n",
      "Epoch [2], Loss: 0.0645\n",
      "Epoch [2], Loss: 0.1435\n",
      "Epoch [2], Loss: 0.8732\n",
      "Epoch [2], Loss: 0.0590\n",
      "Epoch [2], Loss: 0.2703\n",
      "Epoch [2], Loss: 0.9224\n",
      "Epoch [2], Loss: 0.2208\n",
      "Epoch [2], Loss: 0.1127\n",
      "Epoch [2], Loss: 0.1116\n",
      "Epoch [2], Loss: 0.5228\n",
      "Epoch [2], Loss: 0.0999\n",
      "Epoch [2], Loss: 0.2432\n",
      "Epoch [2], Loss: 1.2302\n",
      "Epoch [2], Loss: 0.1037\n",
      "Epoch [2], Loss: 0.1130\n",
      "Epoch [2], Loss: 0.6514\n",
      "Epoch [2], Loss: 0.1524\n",
      "Epoch [2], Loss: 0.4591\n",
      "Epoch [2], Loss: 0.1722\n",
      "Epoch [2], Loss: 0.3033\n",
      "Epoch [2], Loss: 0.2974\n",
      "Epoch [2], Loss: 1.1817\n",
      "Epoch [2], Loss: 0.3280\n",
      "Epoch [2], Loss: 0.9233\n",
      "Epoch [2], Loss: 0.0732\n",
      "Epoch [2], Loss: 0.0716\n",
      "Epoch [2], Loss: 0.1904\n",
      "Epoch [2], Loss: 0.7877\n",
      "Epoch [2], Loss: 0.2758\n",
      "Epoch [2], Loss: 0.1054\n",
      "Epoch [2], Loss: 1.0492\n",
      "Epoch [2], Loss: 0.6303\n",
      "Epoch [2], Loss: 0.3441\n",
      "Epoch [2], Loss: 0.1971\n",
      "Epoch [2], Loss: 0.5297\n",
      "Epoch [2], Loss: 0.2560\n",
      "Epoch [2], Loss: 0.5167\n",
      "Epoch [2], Loss: 0.0739\n",
      "Epoch [2], Loss: 0.7766\n",
      "Epoch [2], Loss: 1.5877\n",
      "Epoch [2], Loss: 0.1290\n",
      "Epoch [2], Loss: 0.1824\n",
      "Epoch [2], Loss: 0.3079\n",
      "Epoch [2], Loss: 0.4397\n",
      "Epoch [2], Loss: 0.2151\n",
      "Epoch [2], Loss: 0.1243\n",
      "Epoch [2], Loss: 0.2582\n",
      "Epoch [2], Loss: 0.3069\n",
      "Epoch [2], Loss: 0.1526\n",
      "Epoch [2], Loss: 0.9610\n",
      "Epoch [2], Loss: 0.0994\n",
      "Epoch [2], Loss: 0.7917\n",
      "Epoch [2], Loss: 1.1356\n",
      "Epoch [2], Loss: 1.1954\n",
      "Epoch [2], Loss: 0.2335\n",
      "Epoch [2], Loss: 0.2675\n",
      "Epoch [2], Loss: 0.1893\n",
      "Epoch [2], Loss: 0.1771\n",
      "Epoch [2], Loss: 0.2546\n",
      "Epoch [2], Loss: 0.6290\n",
      "Epoch [2], Loss: 0.1380\n",
      "Epoch [2], Loss: 0.1595\n",
      "Epoch [2], Loss: 0.1575\n",
      "Epoch [2], Loss: 1.5885\n",
      "Epoch [2], Loss: 0.1301\n",
      "Epoch [2], Loss: 0.1743\n",
      "Epoch [2], Loss: 0.1785\n",
      "Epoch [2], Loss: 1.0795\n",
      "Epoch [2], Loss: 0.0929\n",
      "Epoch [2], Loss: 0.1585\n",
      "Epoch [2], Loss: 0.0991\n",
      "Epoch [2], Loss: 0.1828\n",
      "Epoch [2], Loss: 1.5205\n",
      "Epoch [2], Loss: 0.7785\n",
      "Epoch [2], Loss: 0.1052\n",
      "Epoch [2], Loss: 0.1066\n",
      "Epoch [2], Loss: 0.9942\n",
      "Epoch [2], Loss: 0.2124\n",
      "Epoch [2], Loss: 1.2660\n",
      "Epoch [2], Loss: 0.5678\n",
      "Epoch [2], Loss: 0.1554\n",
      "Epoch [2], Loss: 0.2216\n",
      "Epoch [2], Loss: 0.8714\n",
      "Epoch [2], Loss: 0.0905\n",
      "Epoch [2], Loss: 0.3122\n",
      "Epoch [2], Loss: 0.3300\n",
      "Epoch [2], Loss: 0.4507\n",
      "Epoch [2], Loss: 0.2819\n",
      "Epoch [2], Loss: 0.1418\n",
      "Epoch [2], Loss: 0.0710\n",
      "Epoch [2], Loss: 0.2837\n",
      "Epoch [2], Loss: 0.1474\n",
      "Epoch [2], Loss: 0.2868\n",
      "Epoch [2], Loss: 0.6003\n",
      "Epoch [2], Loss: 0.8928\n",
      "Epoch [2], Loss: 0.1411\n",
      "Epoch [2], Loss: 0.9224\n",
      "Epoch [2], Loss: 0.2356\n",
      "Epoch [2], Loss: 0.1226\n",
      "Epoch [2], Loss: 0.2009\n",
      "Epoch [2], Loss: 0.1419\n",
      "Epoch [2], Loss: 0.1215\n",
      "Epoch [2], Loss: 0.0867\n",
      "Epoch [2], Loss: 0.5137\n",
      "Epoch [2], Loss: 0.9148\n",
      "Epoch [2], Loss: 0.1413\n",
      "Epoch [2], Loss: 0.7511\n",
      "Epoch [2], Loss: 0.8346\n",
      "Epoch [2], Loss: 0.8482\n",
      "Epoch [2], Loss: 0.2247\n",
      "Epoch [2], Loss: 0.1206\n",
      "Epoch [2], Loss: 0.8250\n",
      "Epoch [2], Loss: 0.2979\n",
      "Epoch [2], Loss: 0.1073\n",
      "Epoch [2], Loss: 0.1610\n",
      "Epoch [2], Loss: 0.1498\n",
      "Epoch [2], Loss: 0.1717\n",
      "Epoch [2], Loss: 0.0810\n",
      "Epoch [2], Loss: 0.1476\n",
      "Epoch [2], Loss: 0.2958\n",
      "Epoch [2], Loss: 1.1165\n",
      "Epoch [2], Loss: 0.0604\n",
      "Epoch [2], Loss: 0.5386\n",
      "Epoch [2], Loss: 1.0508\n",
      "Epoch [2], Loss: 0.2378\n",
      "Epoch [2], Loss: 0.1298\n",
      "Epoch [2], Loss: 0.1696\n",
      "Epoch [2], Loss: 0.1230\n",
      "Epoch [2], Loss: 0.1388\n",
      "Epoch [2], Loss: 0.3168\n",
      "Epoch [2], Loss: 1.1112\n",
      "Epoch [2], Loss: 0.1673\n",
      "Epoch [2], Loss: 0.7750\n",
      "Epoch [2], Loss: 1.0180\n",
      "Epoch [2], Loss: 0.8685\n",
      "Epoch [2], Loss: 0.4725\n",
      "Epoch [2], Loss: 0.2289\n",
      "Epoch [2], Loss: 0.7501\n",
      "Epoch [2], Loss: 0.1379\n",
      "Epoch [2], Loss: 0.1768\n",
      "Epoch [2], Loss: 0.4573\n",
      "Epoch [2], Loss: 0.2620\n",
      "Epoch [2], Loss: 0.3695\n",
      "Epoch [2], Loss: 0.3835\n",
      "Epoch [2], Loss: 0.2744\n",
      "Epoch [2], Loss: 0.1886\n",
      "Epoch [2], Loss: 0.2633\n",
      "Epoch [2], Loss: 0.5793\n",
      "Epoch [2], Loss: 0.5191\n",
      "Epoch [2], Loss: 0.5091\n",
      "Epoch [2], Loss: 0.8234\n",
      "Epoch [2], Loss: 0.5512\n",
      "Epoch [2], Loss: 1.1585\n",
      "Epoch [2], Loss: 0.0762\n",
      "Epoch [2], Loss: 0.3866\n",
      "Epoch [2], Loss: 0.1724\n",
      "Epoch [2], Loss: 0.4701\n",
      "Epoch [2], Loss: 0.2566\n",
      "Epoch [2], Loss: 1.0142\n",
      "Epoch [2], Loss: 0.9247\n",
      "Epoch [2], Loss: 0.3084\n",
      "Epoch [2], Loss: 0.2632\n",
      "Epoch [2], Loss: 0.8432\n",
      "Epoch [2], Loss: 0.4818\n",
      "Epoch [2], Loss: 0.4449\n",
      "Epoch [2], Loss: 0.7020\n",
      "Epoch [2], Loss: 0.2777\n",
      "Epoch [2], Loss: 0.2726\n",
      "Epoch [2], Loss: 0.5179\n",
      "Epoch [2], Loss: 0.4847\n",
      "Epoch [2], Loss: 0.1005\n",
      "Epoch [2], Loss: 0.1251\n",
      "Epoch [2], Loss: 0.1637\n",
      "Epoch [2], Loss: 0.7204\n",
      "Epoch [2], Loss: 0.8294\n",
      "Epoch [2], Loss: 0.0828\n",
      "Epoch [2], Loss: 0.9903\n",
      "Epoch [2], Loss: 0.5040\n",
      "Epoch [2], Loss: 1.1850\n",
      "Epoch [2], Loss: 0.5945\n",
      "Epoch [2], Loss: 1.4258\n",
      "Epoch [2], Loss: 0.1145\n",
      "Epoch [2], Loss: 0.1936\n",
      "Epoch [2], Loss: 0.1206\n",
      "Epoch [2], Loss: 0.0729\n",
      "Epoch [2], Loss: 0.8101\n",
      "Epoch [2], Loss: 0.1392\n",
      "Epoch [2], Loss: 0.0904\n",
      "Epoch [2], Loss: 0.2180\n",
      "Epoch [2], Loss: 0.3123\n",
      "Epoch [2], Loss: 0.1018\n",
      "Epoch [2], Loss: 0.2905\n",
      "Epoch [2], Loss: 0.2245\n",
      "Epoch [2], Loss: 0.3931\n",
      "Epoch [2], Loss: 0.1922\n",
      "Epoch [2], Loss: 1.2595\n",
      "Epoch [2], Loss: 1.2773\n",
      "Epoch [2], Loss: 0.1530\n",
      "Epoch [2], Loss: 0.2429\n",
      "Epoch [2], Loss: 0.5093\n",
      "Epoch [2], Loss: 0.1884\n",
      "Epoch [2], Loss: 0.9978\n",
      "Epoch [2], Loss: 0.1490\n",
      "Epoch [2], Loss: 0.7299\n",
      "Epoch [2], Loss: 0.6774\n",
      "Epoch [2], Loss: 0.1412\n",
      "Epoch [2], Loss: 0.7101\n",
      "Epoch [2], Loss: 0.1566\n",
      "Epoch [2], Loss: 0.8763\n",
      "Epoch [2], Loss: 0.6888\n",
      "Epoch [2], Loss: 0.7561\n",
      "Epoch [2], Loss: 0.0975\n",
      "Epoch [2], Loss: 0.2201\n",
      "Epoch [2], Loss: 0.4510\n",
      "Epoch [2], Loss: 0.1972\n",
      "Epoch [2], Loss: 0.1226\n",
      "Epoch [2], Loss: 0.1990\n",
      "Epoch [2], Loss: 0.7983\n",
      "Epoch [2], Loss: 0.3545\n",
      "Epoch [2], Loss: 0.0798\n",
      "Epoch [2], Loss: 1.1612\n",
      "Epoch [2], Loss: 0.4590\n",
      "Epoch [2], Loss: 1.0085\n",
      "Epoch [2], Loss: 0.3087\n",
      "Epoch [2], Loss: 0.6676\n",
      "Epoch [2], Loss: 0.2966\n",
      "Epoch [2], Loss: 0.4094\n",
      "Epoch [2], Loss: 0.9429\n",
      "Epoch [2], Loss: 0.3335\n",
      "Epoch [2], Loss: 0.1620\n",
      "Epoch [2], Loss: 0.3287\n",
      "Epoch [2], Loss: 0.2606\n",
      "Epoch [2], Loss: 0.2672\n",
      "Epoch [2], Loss: 0.1132\n",
      "Epoch [2], Loss: 1.0570\n",
      "Epoch [2], Loss: 0.1118\n",
      "Epoch [2], Loss: 0.2299\n",
      "Epoch [2], Loss: 0.9929\n",
      "Epoch [2], Loss: 0.3541\n",
      "Epoch [2], Loss: 0.3402\n",
      "Epoch [2], Loss: 0.6268\n",
      "Epoch [2], Loss: 0.2263\n",
      "Epoch [2], Loss: 0.1462\n",
      "Epoch [2], Loss: 0.8896\n",
      "Epoch [2], Loss: 0.9318\n",
      "Epoch [2], Loss: 0.6876\n",
      "Epoch [2], Loss: 0.0896\n",
      "Epoch [2], Loss: 0.2480\n",
      "Epoch [2], Loss: 0.8433\n",
      "Epoch [2], Loss: 0.7538\n",
      "Epoch [2], Loss: 0.1395\n",
      "Epoch [2], Loss: 0.3217\n",
      "Epoch [2], Loss: 0.3693\n",
      "Epoch [2], Loss: 0.2655\n",
      "Epoch [2], Loss: 0.7563\n",
      "Epoch [2], Loss: 0.2768\n",
      "Epoch [2], Loss: 0.6951\n",
      "Epoch [2], Loss: 0.7598\n",
      "Epoch [2], Loss: 0.6940\n",
      "Epoch [2], Loss: 0.1233\n",
      "Epoch [2], Loss: 0.2338\n",
      "Epoch [2], Loss: 0.1615\n",
      "Epoch [2], Loss: 0.3708\n",
      "Epoch [2], Loss: 0.0969\n",
      "Epoch [2], Loss: 0.0900\n",
      "Epoch [2], Loss: 0.1208\n",
      "Epoch [2], Loss: 0.1426\n",
      "Epoch [2], Loss: 0.1939\n",
      "Epoch [2], Loss: 1.3017\n",
      "Epoch [2], Loss: 1.1375\n",
      "Epoch [2], Loss: 1.4931\n",
      "Epoch [2], Loss: 0.1706\n",
      "Epoch [2], Loss: 0.2364\n",
      "Epoch [2], Loss: 0.6807\n",
      "Epoch [2], Loss: 0.5694\n",
      "Epoch [2], Loss: 0.6707\n",
      "Epoch [2], Loss: 1.5067\n",
      "Epoch [2], Loss: 0.6642\n",
      "Epoch [2], Loss: 0.9445\n",
      "Epoch [2], Loss: 0.1529\n",
      "Epoch [2], Loss: 0.5946\n",
      "Epoch [2], Loss: 0.3517\n",
      "Epoch [2], Loss: 0.3619\n",
      "Epoch [2], Loss: 0.1202\n",
      "Epoch [2], Loss: 0.1263\n",
      "Epoch [2], Loss: 0.3092\n",
      "Epoch [2], Loss: 0.3598\n",
      "Epoch [2], Loss: 0.7708\n",
      "Epoch [2], Loss: 0.3517\n",
      "Epoch [2], Loss: 0.3160\n",
      "Epoch [2], Loss: 0.3524\n",
      "Epoch [2], Loss: 0.1261\n",
      "Epoch [2], Loss: 0.1423\n",
      "Epoch [2], Loss: 0.0706\n",
      "Epoch [2], Loss: 0.7328\n",
      "Epoch [2], Loss: 0.1197\n",
      "Epoch [2], Loss: 0.9999\n",
      "Epoch [2], Loss: 0.2598\n",
      "Epoch [2], Loss: 0.2228\n",
      "Epoch [2], Loss: 0.6461\n",
      "Epoch [2], Loss: 0.1852\n",
      "Epoch [2], Loss: 0.1413\n",
      "Epoch [2], Loss: 0.5150\n",
      "Epoch [2], Loss: 0.1343\n",
      "Epoch [2], Loss: 0.2202\n",
      "Epoch [2], Loss: 0.1220\n",
      "Epoch [2], Loss: 0.2514\n",
      "Epoch [2], Loss: 0.9844\n",
      "Epoch [2], Loss: 0.2570\n",
      "Epoch [2], Loss: 0.5765\n",
      "Epoch [2], Loss: 0.2003\n",
      "Epoch [2], Loss: 0.2219\n",
      "Epoch [2], Loss: 0.1403\n",
      "Epoch [2], Loss: 0.1045\n",
      "Epoch [2], Loss: 0.1428\n",
      "Epoch [2], Loss: 1.3337\n",
      "Epoch [2], Loss: 0.2448\n",
      "Epoch [2], Loss: 0.0552\n",
      "Epoch [2], Loss: 0.1385\n",
      "Epoch [2], Loss: 0.9924\n",
      "Epoch [2], Loss: 0.1006\n",
      "Epoch [2], Loss: 0.1004\n",
      "Epoch [2], Loss: 0.2150\n",
      "Epoch [2], Loss: 1.1642\n",
      "Epoch [2], Loss: 0.1192\n",
      "Epoch [2], Loss: 0.1099\n",
      "Epoch [2], Loss: 0.5472\n",
      "Epoch [2], Loss: 0.1990\n",
      "Epoch [2], Loss: 0.0993\n",
      "Epoch [2], Loss: 0.1026\n",
      "Epoch [2], Loss: 0.1844\n",
      "Epoch [2], Loss: 0.2065\n",
      "Epoch [2], Loss: 0.5956\n",
      "Epoch [2], Loss: 1.1417\n",
      "Epoch [2], Loss: 0.1230\n",
      "Epoch [2], Loss: 0.9563\n",
      "Epoch [2], Loss: 0.4858\n",
      "Epoch [2], Loss: 0.4455\n",
      "Epoch [2], Loss: 0.0809\n",
      "Epoch [2], Loss: 0.1008\n",
      "Epoch [2], Loss: 0.2242\n",
      "Epoch [2], Loss: 0.8899\n",
      "Epoch [2], Loss: 0.6559\n",
      "Epoch [2], Loss: 0.1284\n",
      "Epoch [2], Loss: 0.5048\n",
      "Epoch [2], Loss: 1.1975\n",
      "Epoch [2], Loss: 0.5113\n",
      "Epoch [2], Loss: 0.2304\n",
      "Epoch [2], Loss: 0.1979\n",
      "Epoch [2], Loss: 0.4055\n",
      "Epoch [2], Loss: 0.0951\n",
      "Epoch [2], Loss: 0.5230\n",
      "Epoch [2], Loss: 0.0669\n",
      "Epoch [2], Loss: 1.2114\n",
      "Epoch [2], Loss: 0.2122\n",
      "Epoch [2], Loss: 0.9442\n",
      "Epoch [2], Loss: 0.1148\n",
      "Epoch [2], Loss: 0.2160\n",
      "Epoch [2], Loss: 0.0995\n",
      "Epoch [2], Loss: 0.1391\n",
      "Epoch [2], Loss: 0.0930\n",
      "Epoch [2], Loss: 1.1045\n",
      "Epoch [2], Loss: 0.6284\n",
      "Epoch [2], Loss: 0.7146\n",
      "Epoch [2], Loss: 0.5941\n",
      "Epoch [2], Loss: 0.2887\n",
      "Epoch [2], Loss: 0.1583\n",
      "Epoch [2], Loss: 0.1393\n",
      "Epoch [2], Loss: 0.3005\n",
      "Epoch [2], Loss: 0.7172\n",
      "Epoch [2], Loss: 0.1949\n",
      "Epoch [2], Loss: 0.7464\n",
      "Epoch [2], Loss: 0.7161\n",
      "Epoch [2], Loss: 0.3630\n",
      "Epoch [2], Loss: 0.1316\n",
      "Epoch [2], Loss: 0.5947\n",
      "Epoch [2], Loss: 0.7632\n",
      "Epoch [2], Loss: 1.0932\n",
      "Epoch [2], Loss: 0.7207\n",
      "Epoch [2], Loss: 0.0611\n",
      "Epoch [2], Loss: 0.0641\n",
      "Epoch [2], Loss: 0.8600\n",
      "Epoch [2], Loss: 1.0521\n",
      "Epoch [2], Loss: 0.9954\n",
      "Epoch [2], Loss: 0.5101\n",
      "Epoch [2], Loss: 0.2630\n",
      "Epoch [2], Loss: 0.6806\n",
      "Epoch [2], Loss: 0.7838\n",
      "Epoch [2], Loss: 0.4538\n",
      "Epoch [2], Loss: 0.5215\n",
      "Epoch [2], Loss: 0.2266\n",
      "Epoch [2], Loss: 0.7334\n",
      "Epoch [2], Loss: 0.1935\n",
      "Epoch [2], Loss: 0.3655\n",
      "Epoch [2], Loss: 0.1429\n",
      "Epoch [2], Loss: 0.3564\n",
      "Epoch [2], Loss: 0.5041\n",
      "Epoch [2], Loss: 0.2480\n",
      "Epoch [2], Loss: 0.5578\n",
      "Epoch [2], Loss: 0.1200\n",
      "Epoch [2], Loss: 0.1924\n",
      "Epoch [2], Loss: 0.8418\n",
      "Epoch [2], Loss: 0.1108\n",
      "Epoch [2], Loss: 0.1682\n",
      "Epoch [2], Loss: 0.4417\n",
      "Epoch [2], Loss: 1.0426\n",
      "Epoch [2], Loss: 0.2562\n",
      "Epoch [2], Loss: 0.7403\n",
      "Epoch [2], Loss: 0.3089\n",
      "Epoch [2], Loss: 0.1302\n",
      "Epoch [2], Loss: 0.5147\n",
      "Epoch [2], Loss: 1.0294\n",
      "Epoch [2], Loss: 1.0693\n",
      "Epoch [2], Loss: 0.3675\n",
      "Epoch [2], Loss: 1.3103\n",
      "Epoch [2], Loss: 0.6931\n",
      "Epoch [2], Loss: 0.5949\n",
      "Epoch [2], Loss: 0.4002\n",
      "Epoch [2], Loss: 0.4154\n",
      "Epoch [2], Loss: 0.4246\n",
      "Epoch [2], Loss: 0.1175\n",
      "Epoch [2], Loss: 0.2167\n",
      "Epoch [2], Loss: 0.1916\n",
      "Epoch [2], Loss: 0.2571\n",
      "Epoch [2], Loss: 0.0870\n",
      "Epoch [2], Loss: 1.0866\n",
      "Epoch [2], Loss: 0.1513\n",
      "Epoch [2], Loss: 0.4022\n",
      "Epoch [2], Loss: 0.2509\n",
      "Epoch [2], Loss: 0.6719\n",
      "Epoch [2], Loss: 0.4977\n",
      "Epoch [2], Loss: 0.4875\n",
      "Epoch [2], Loss: 0.5856\n",
      "Epoch [2], Loss: 0.2549\n",
      "Epoch [2], Loss: 0.4441\n",
      "Epoch [2], Loss: 0.2471\n",
      "Epoch [2], Loss: 0.6082\n",
      "Epoch [2], Loss: 0.4426\n",
      "Epoch [2], Loss: 0.4160\n",
      "Epoch [2], Loss: 0.6590\n",
      "Epoch [2], Loss: 0.6004\n",
      "Epoch [2], Loss: 0.5191\n",
      "Epoch [2], Loss: 0.5516\n",
      "Epoch [2], Loss: 0.2160\n",
      "Epoch [2], Loss: 0.2416\n",
      "Epoch [2], Loss: 0.2361\n",
      "Epoch [2], Loss: 0.1967\n",
      "Epoch [2], Loss: 0.1413\n",
      "Epoch [2], Loss: 0.2072\n",
      "Epoch [2], Loss: 0.8068\n",
      "Epoch [2], Loss: 0.1344\n",
      "Epoch [2], Loss: 0.1897\n",
      "Epoch [2], Loss: 0.2339\n",
      "Epoch [2], Loss: 0.8949\n",
      "Epoch [2], Loss: 0.1667\n",
      "Epoch [2], Loss: 0.1334\n",
      "Epoch [2], Loss: 0.1051\n",
      "Epoch [2], Loss: 0.0720\n",
      "Epoch [2], Loss: 0.8773\n",
      "Epoch [2], Loss: 0.0661\n",
      "Epoch [2], Loss: 0.9211\n",
      "Epoch [2], Loss: 1.2083\n",
      "Epoch [2], Loss: 0.1294\n",
      "Epoch [2], Loss: 1.0561\n",
      "Epoch [2], Loss: 0.1022\n",
      "Epoch [2], Loss: 1.2919\n",
      "Epoch [2], Loss: 0.8339\n",
      "Epoch [2], Loss: 0.6313\n",
      "Epoch [2], Loss: 1.0754\n",
      "Epoch [2], Loss: 0.4894\n",
      "Epoch [2], Loss: 0.1465\n",
      "Epoch [2], Loss: 0.8640\n",
      "Epoch [2], Loss: 0.9770\n",
      "Epoch [2], Loss: 0.2762\n",
      "Epoch [2], Loss: 0.2375\n",
      "Epoch [2], Loss: 0.1675\n",
      "Epoch [2], Loss: 0.6401\n",
      "Epoch [2], Loss: 0.3111\n",
      "Epoch [2], Loss: 0.6012\n",
      "Epoch [2], Loss: 0.5915\n",
      "Epoch [2], Loss: 0.8302\n",
      "Epoch [2], Loss: 0.3198\n",
      "Epoch [2], Loss: 0.6450\n",
      "Epoch [2], Loss: 0.3907\n",
      "Epoch [2], Loss: 0.2574\n",
      "Epoch [2], Loss: 0.5145\n",
      "Epoch [2], Loss: 0.1131\n",
      "Epoch [2], Loss: 0.5461\n",
      "Epoch [2], Loss: 0.4889\n",
      "Epoch [2], Loss: 0.4836\n",
      "Epoch [2], Loss: 0.2250\n",
      "Epoch [2], Loss: 0.7327\n",
      "Epoch [2], Loss: 0.2431\n",
      "Epoch [2], Loss: 0.3903\n",
      "Epoch [2], Loss: 1.2325\n",
      "Epoch [2], Loss: 0.2145\n",
      "Epoch [2], Loss: 0.2159\n",
      "Epoch [2], Loss: 1.1113\n",
      "Epoch [2], Loss: 0.1969\n",
      "Epoch [2], Loss: 0.3938\n",
      "Epoch [2], Loss: 0.3179\n",
      "Epoch [2], Loss: 0.3175\n",
      "Epoch [2], Loss: 0.2648\n",
      "Epoch [2], Loss: 0.6550\n",
      "Epoch [2], Loss: 0.1594\n",
      "Epoch [2], Loss: 0.1582\n",
      "Epoch [2], Loss: 0.2235\n",
      "Epoch [2], Loss: 0.1887\n",
      "Epoch [2], Loss: 0.2956\n",
      "Epoch [2], Loss: 0.1039\n",
      "Epoch [2], Loss: 0.5500\n",
      "Epoch [2], Loss: 0.1290\n",
      "Epoch [2], Loss: 0.7811\n",
      "Epoch [2], Loss: 0.1081\n",
      "Epoch [2], Loss: 0.0871\n",
      "Epoch [2], Loss: 0.1737\n",
      "Epoch [2], Loss: 0.0695\n",
      "Epoch [2], Loss: 0.1309\n",
      "Epoch [2], Loss: 0.9434\n",
      "Epoch [2], Loss: 0.7589\n",
      "Epoch [2], Loss: 0.6866\n",
      "Epoch [2], Loss: 0.5494\n",
      "Epoch [2], Loss: 0.0943\n",
      "Epoch [2], Loss: 0.1410\n",
      "Epoch [2], Loss: 0.1834\n",
      "Epoch [2], Loss: 0.1831\n",
      "Epoch [2], Loss: 0.3762\n",
      "Epoch [2], Loss: 0.1534\n",
      "Epoch [2], Loss: 0.1435\n",
      "Epoch [2], Loss: 0.1165\n",
      "Epoch [2], Loss: 0.9663\n",
      "Epoch [2], Loss: 0.0868\n",
      "Epoch [2], Loss: 0.1606\n",
      "Epoch [2], Loss: 0.1287\n",
      "Epoch [2], Loss: 0.1682\n",
      "Epoch [2], Loss: 0.0718\n",
      "Epoch [2], Loss: 0.0911\n",
      "Epoch [2], Loss: 0.5022\n",
      "Epoch [2], Loss: 0.1575\n",
      "Epoch [2], Loss: 0.4935\n",
      "Epoch [2], Loss: 0.0678\n",
      "Epoch [2], Loss: 0.0999\n",
      "Epoch [2], Loss: 0.9002\n",
      "Epoch [2], Loss: 0.1286\n",
      "Epoch [2], Loss: 0.1414\n",
      "Epoch [2], Loss: 0.1108\n",
      "Epoch [2], Loss: 1.2238\n",
      "Epoch [2], Loss: 0.1182\n",
      "Epoch [2], Loss: 0.1321\n",
      "Epoch [2], Loss: 0.9467\n",
      "Epoch [2], Loss: 0.1418\n",
      "Epoch [2], Loss: 0.1476\n",
      "Epoch [2], Loss: 1.0392\n",
      "Epoch [2], Loss: 0.6335\n",
      "Epoch [2], Loss: 0.3232\n",
      "Epoch [2], Loss: 0.3182\n",
      "Epoch [2], Loss: 0.6203\n",
      "Epoch [2], Loss: 0.2488\n",
      "Epoch [2], Loss: 0.2289\n",
      "Epoch [2], Loss: 0.2062\n",
      "Epoch [2], Loss: 0.1236\n",
      "Epoch [2], Loss: 0.1156\n",
      "Epoch [2], Loss: 0.5446\n",
      "Epoch [2], Loss: 0.7704\n",
      "Epoch [2], Loss: 0.0703\n",
      "Epoch [2], Loss: 0.1322\n",
      "Epoch [2], Loss: 0.1725\n",
      "Epoch [2], Loss: 0.8510\n",
      "Epoch [2], Loss: 0.7423\n",
      "Epoch [2], Loss: 0.0738\n",
      "Epoch [2], Loss: 0.1926\n",
      "Epoch [2], Loss: 0.7582\n",
      "Epoch [2], Loss: 1.1785\n",
      "Epoch [2], Loss: 1.1880\n",
      "Epoch [2], Loss: 0.0562\n",
      "Epoch [2], Loss: 0.9743\n",
      "Epoch [2], Loss: 0.8515\n",
      "Epoch [2], Loss: 0.4638\n",
      "Epoch [2], Loss: 1.0683\n",
      "Epoch [2], Loss: 0.5339\n",
      "Epoch [2], Loss: 0.5276\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    loss_history = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        processed_batch = process_batch_for_landmarks(inputs)\n",
    "        landmarks = extract_landmarks_from_batch(processed_batch, hands)\n",
    "        landmarks = torch.tensor(landmarks).float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(landmarks)\n",
    "        # outputs = outputs[:, 1]\n",
    "        # labels = labels.view(-1).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        print(f'Epoch [{epoch+1}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(\"D:\", \"IPN_Hand\",\"annotations-20231128T085307Z-001\", \"annotations\", \"Annot_TestList.txt\")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "test_set = GestureDataset(hdf5_path, file, label_all_gestures=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = tensor([1]) \t gt = tensor([1])\n",
      "y = tensor([1]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([1]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([1]) \t gt = tensor([1])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([1]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([1]) \t gt = tensor([1])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([1]) \t gt = tensor([1])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([1]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([1]) \t gt = tensor([1])\n",
      "y = tensor([1]) \t gt = tensor([1])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n",
      "y = tensor([0]) \t gt = tensor([0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    processed_batch = process_batch_for_landmarks(inputs)\n",
    "    landmarks = extract_landmarks_from_batch(processed_batch, hands)\n",
    "    landmarks = torch.tensor(landmarks).float()\n",
    "\n",
    "    outputs = model(landmarks)\n",
    "\n",
    "    y = torch.argmax(outputs, dim=1)\n",
    "    print(f\"y = {y} \\t gt = {labels}\")\n",
    "\n",
    "    if i == 30:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kenan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train.train_lstm_lm_det import main\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = os.path.join(\"D:\", \"IPN_Hand\",\"annotations-20231128T085307Z-001\", \"annotations\", \"Annot_TestList.txt\")\n",
    "train = os.path.join(\"D:\", \"IPN_Hand\",\"annotations-20231128T085307Z-001\", \"annotations\", \"Annot_TrainList.txt\")\n",
    "# frame_folders = os.path.join(\".\", \"IPN_Hand\", \"frames\")\n",
    "hdf5_path = os.path.join(\"D:\", \"IPN_Hand\", \"hand_gestures.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6600000262260437, 'precision': 0.6600000262260437, 'recall': 0.6600000262260437}\n"
     ]
    }
   ],
   "source": [
    "main(hdf5_path, train, test, \"lstm_det_local.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
